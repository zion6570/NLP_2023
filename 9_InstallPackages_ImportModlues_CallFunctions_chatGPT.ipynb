{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zion6570/NLP_2023/blob/main/9_InstallPackages_ImportModlues_CallFunctions_chatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎁 Python Library\n",
        "  - Python modues 을 계층적인 디렉토리 형태로 구성\n",
        "  - **!pip**: Package manager\n",
        "  - **!pip install <font color = 'red'> NameLibrary**\n",
        "> A <font color = 'red'> **Python library**</font> refers to a collection of modules or functions that provide specific functionality, often focused on a particular domain or purpose. Libraries can be used to extend the capabilities of Python by providing pre-written code that can be imported and used in your own programs. Examples of popular Python libraries include NumPy for numerical computing, pandas for data manipulation and analysis, and requests for making HTTP requests. On the other hand, a <font color = 'blue'> **Python package**</font> is a way of organizing related modules into a directory hierarchy. A package is essentially a directory that contains one or more Python module files, along with an optional __init__.py file that signifies it as a package. Packages help to organize and structure large codebases by grouping related functionality together. They can also contain sub-packages, creating a nested structure.\n",
        "\n",
        "# 🎒🎒 Python Moduess\n",
        "  - Python functions 로 구성\n",
        "  - **from 페키지이름 import 모듈이름**\n",
        "  - **import** **페키지.모듈이름 <font color='green'>[외부 페키지 경우]**</font>\n",
        "  - **import 모듈이름 <font color='purple'>[Python 내장 페키지 경우]**</font>\n",
        "  - **import 모듈이름 as Abbreviation**\n",
        "\n",
        "\n",
        "# 🏀 ⚽ ⚾ 🎾 Python functions\n",
        "  - 외부 모듈에 있는 함수\n",
        "  - import 모듈이름.함수이름()\n",
        "  - from 모듈이름 import 함수이름\n",
        "\n",
        "### For you information, check out **Python Module Index**\n",
        "* [Visit Colab documentation]((https://docs.python.org/3/py-modindex.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "aCsaIGXK73ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<font color = 'purple'> **👀 Install Python Libraries** ⤵️"
      ],
      "metadata": {
        "id": "nR3MiBaAM--d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 🐹 👀 🐾 The **Pandas** is a popular open-source library for data manipulation and analysis. It offers data structures like **Series and DataFrame for handling structured data**. With powerful functionalities, it enables tasks such as <font color = 'red'>**indexing, filtering, grouping, and merging data**</font>. Pandas supports various file formats and integrates well with other libraries like NumPy and Matplotlib. It provides an intuitive and efficient way to work with large datasets in Python.\n",
        "\n",
        "!pip install pandas"
      ],
      "metadata": {
        "id": "9olyrXb-H-mq",
        "cellView": "form",
        "outputId": "b87cec09-ce3e-4258-ee4e-5931835b9d26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "N3uQ4Tg47yjh",
        "outputId": "c73e286d-a96a-4d58-bf13-4076a3cfe997",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 🐹 👀 🐾 The **scikit-learn**, often referred to as sklearn, is a widely used **machine learning library**. It provides a comprehensive collection of tools and algorithms for various machine learning tasks such as <font color = 'red'>**classification, regression, clustering, and dimensionality reduction**</font>. With a consistent and user-friendly API, scikit-learn simplifies the process of building machine learning models. It supports data preprocessing, feature selection, model evaluation, and model tuning. The library also offers helpful utilities for handling datasets and implementing machine learning workflows. Overall, scikit-learn is a valuable resource for both beginners and experienced practitioners in the field of machine learning.\n",
        "\n",
        "!pip install scikit-learn #corpus-toolkit 패키지에 하이픈 있음"
      ],
      "metadata": {
        "id": "sD-yK0LIICxj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 🐹 👀 🐾 The **Matplotlib** is a popular plotting library for Python. It provides a flexible and comprehensive set of tools for creating various types of plots and visualizations. With a simple and intuitive interface, Matplotlib allows customization of plot appearance, axes, labels, and styles. It supports <font color = 'red'>**line plots, scatter plots, bar charts, histograms**</font>, and more. Matplotlib *integrates well with NumPy and Pandas for data manipulation and analysis*. It is widely used for data exploration, presentation, and publication-quality visualizations in scientific computing and data analysis.\n",
        "\n",
        "!pip install matplot"
      ],
      "metadata": {
        "id": "Yo1rFBHOIOrA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 🐹 👀 🐾 The **NLTK** (Natural Language Toolkit) is a powerful library for natural language processing (NLP) tasks. It provides tools and resources for tasks like <font color = 'red'>**tokenization, stemming, tagging, parsing, and sentiment analysis**</font>. NLTK offers a wide range of corpora and lexical resources for linguistic analysis. It supports **text classification, text generation, and language modeling**. NLTK includes pre-trained models and algorithms for various NLP tasks, making it suitable for both beginners and advanced users in the field of NLP. Overall, NLTK is a valuable resource for working with *human language data and performing NLP tasks in Python*.\n",
        "\n",
        "!pip install nltk"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LKUrjZgzza9Q",
        "outputId": "1942e5b8-250b-4b10-8305-bc41c43fc5c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 🐹 👀 🐾 **Corpus-toolkit** package grew out of courses in corpus linguistics and learner corpus research. The toolkit attempts to balance simplicity of use, broad application, and scalability. Common corpus analyses such as <font color = 'red'>**the calculation of word and n-gram frequency and range, keyness, and collocation**</font> are included. In addition, more advanced analyses such as the **identification of dependency bigrams (e.g., verb-direct object combinations) and their frequency, range, and strength of association** are also included.\n",
        "\n",
        "!pip install corpus-toolkit #corpus-toolkit 패키지에 하이픈 있음"
      ],
      "metadata": {
        "id": "aFjO_wBAH5Ql",
        "cellView": "form",
        "outputId": "fb7b5e86-1b0c-4699-8139-431c6a14208f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: corpus-toolkit in /usr/local/lib/python3.10/dist-packages (0.32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize , sent_tokenize\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "itpJH2yP_E9_",
        "outputId": "4447de7b-9050-4d46-d047-d5c1f9382a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<font color = 'brown'> Student's activity** ⤵️\n",
        "\n",
        "### Exercise for <font color = 'red'> installing nltk package, importing its module, and calling its functions\n",
        "\n",
        "**NLTK: Python library**\n",
        "* You are correct! When using **Google Colab**, certain libraries, including **NLTK**, are **pre-installed and available for immediate use** without the need for additional installation. This is because Colab provides a pre-configured environment with several popular libraries and modules already installed, allowing you to import and use them directly in your code. So, in the case of using Colab, you can indeed use the NLTK module without explicitly installing the NLTK library.\n",
        ">\n",
        "* NLTK의 기능을 제대로 사용하기 위해서는 NLTK Data라는 여러 데이터를 추가적으로 설치해야 한다.\n",
        ">\n",
        "* 이를 위해서 파이썬 코드 내에서 import nltk 이후에 nltk.download()라는 코드를 수행하여 설치한다.\n",
        ">\n",
        "* **Reference**\n",
        "  * [wikidocs](https://wikidocs.net/22488)\n"
      ],
      "metadata": {
        "id": "SL1F8WCDKvoN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TPWVgk272SH"
      },
      "outputs": [],
      "source": [
        "!pip install nltk #This step can be skipped since it is pre-installed on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "7dBxHVDoIub9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<font color = 'brown'> Student's activity** ⤵️\n",
        "\n",
        "### Exercise for <font color = 'red'> *importing os module and calling its functions*\n",
        "\n",
        "   -The **os module is a built-in module in Python**, meaning it is available by default in any Python installation. You don't need to install it separately or use any package manager. The os module provides functions for interacting with the operating system, such as accessing files and directories, managing processes, and other system-related tasks. You can import and use the os module in your Python programs without any additional installation steps."
      ],
      "metadata": {
        "id": "hioxpHpfQLjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os                 #Python built-in os module 불러 들이기\n",
        "os.mkdir (\"txtfolder\")    #os 모듈과 mkdir 함수 사이에 period 있음. Check \"txtfolder\" under Files of Colab."
      ],
      "metadata": {
        "id": "8PxL2p05WDin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "text = \"Python programming is a high-level, interpreted programming language known for its simplicity and readability. It emphasizex code readability with its clean syntax, making it easier to write and understand, Python supports multiple programming paradigmsm, including procedural, object-oriented, and functional pogramming. It has a vast standard libaryand a thriving ecosystem of third-party libraries and framewoksm making it suiable for various damains such as web development, data analysis, machine learning, and automation. Python's versatility, ease of use, and extensive community spport have contributed to its popularity among developers of all skill levels.\"\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentence = sent_tokenize(text)\n",
        "print('문장 토큰화: %s' %sentence)"
      ],
      "metadata": {
        "id": "z-a76TKtAr4_",
        "outputId": "a7029c6f-e6aa-4da6-a77d-5fab77e8f751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "문장 토큰화: ['Python programming is a high-level, interpreted programming language known for its simplicity and readability.', 'It emphasizex code readability with its clean syntax, making it easier to write and understand, Python supports multiple programming paradigmsm, including procedural, object-oriented, and functional pogramming.', 'It has a vast standard libaryand a thriving ecosystem of third-party libraries and framewoksm making it suiable for various damains such as web development, data analysis, machine learning, and automation.', \"Python's versatility, ease of use, and extensive community spport have contributed to its popularity among developers of all skill levels.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "text = \"Python programming is a high-level, interpreted programming language known for its simplicity and readability. It emphasizes code readability with its clean syntax, making it easier to write and understand. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It has a vast standard library and a thriving ecosystem of third-party libraries and frameworks, making it suitable for various domains such as web development, data analysis, machine learning, and automation. Python's versatility, ease of use, and extensive community support have contributed to its popularity among developers of all skill levels.\"\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentence = sent_tokenize(text)\n",
        "print('문장 토큰화: %s' %sentence)\n"
      ],
      "metadata": {
        "id": "b0VovEVr2dFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 🐹 👀 🐾 **Lexical-diversity** library is a Python package that provides tools and functions for analyzing the lexical diversity of text.  i) <font color = 'red'>**Type-Token Ratio (TTR)**</font> measures the proportion of unique words (types) in a text compared to the total number of words (tokens). It provides insights into vocabulary richness. ii)  <font color = 'red'>**Moving Standardized Type-Token Ratio (MSTTR)**</font> is a dynamic measure of lexical diversity that takes into account a moving window of text, allowing you to assess diversity over smaller sections of text. iii)  <font color = 'red'>**Moving Average Type-Token Ration (MATTR)**</font> calculates the Type-Token Ratio (TTR) within a sliding window as it moves through the text, and then computes the average of these TTR values over the entire text. The formula for MATTR: MATTR = (1 / N) * ∑(TTR_i).\n",
        "\n",
        "#@markdown 🐹 👀 🐾 Total number of words / Total number of type\n",
        "\n",
        "!pip install lexical-diversity"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VBD9lDWz0Eoj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}